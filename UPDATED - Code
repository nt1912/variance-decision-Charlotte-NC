import os
import openai
import pandas as pd
import re
import json
from PyPDF2 import PdfReader
from transformers import GPT2TokenizerFast

# OpenAI API key
openai.api_key = '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'

# Set the folder path containing the OCR'd PDF files
folder_path = r"C:\Users\nghie\Downloads\Charlotte NC 2019 Decision Sample"

# Initialize GPT2 tokenizer
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

# Define tokens_used variable
tokens_used = 0

# Function to read text from a PDF file using PyPDF2
def read_pdf(file_path):
    pdf_text = ""
    with open(file_path, 'rb') as file:
        pdf_reader = PdfReader(file)
        for page in pdf_reader.pages:
            pdf_text += page.extract_text()

    return pdf_text

# Function to extract the address from text using keyword matching
def extract_address_with_keywords(text):
    keywords = ['ROAD', 'AVENUE', 'AV', 'STREET', 'ST', 'BLVD', 'AVENIDA']
    address_pattern = r'\b\d+\s+(?:\w+\s+)*(?:' + '|'.join(keywords) + r')\b'
    lines = text.split('\n')

    for i, line in enumerate(lines):
        if 'RE:' in line.upper() and i + 1 < len(lines):
            # Search for the address in the next line using the pattern
            match = re.search(address_pattern, lines[i + 1], re.IGNORECASE)

            if match:
                return match.group(0)
    return None

def extract_date_at_top(text):
    # Date pattern in the format: Month Day, Year
    date_pattern = r"(January|February|March|April|May|June|July|August|September|October|November|December)\s\d{1,2},\s\d{4}"
    date_match = re.search(date_pattern, text)
    return date_match.group(0) if date_match else None

def extract_tax_parcel_number(text):
    # Tax parcel number pattern
    tax_pattern = r"TAX PARCEL NUMBER:\s*([\d-]+)"
    tax_match = re.search(tax_pattern, text)
    return tax_match.group(1) if tax_match else None

def complete_text(prompt, temp=0, trys=0, clean=True):
    global tokens_used

    model = "text-davinci-003"
    model_token_limit = 4097

    token_count = len(tokenizer.encode(prompt))
    max_tokens = model_token_limit - round(token_count + 5)

    try:
        response = openai.Completion.create(
            model=model,
            prompt=prompt,
            temperature=temp,
            max_tokens=max_tokens,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        output = response["choices"][0]["text"].strip()
    except Exception as e:
        print(f"Problem with API call: {e}")
        output = "error"

    tokens_used += token_count + len(tokenizer.encode(output))

    if clean:
        # Wrap the output in a JSON string with "output" as the key
        return json.dumps({"output": output})
    else:
        return {"output": output}

def clean_pseudo_json(string, temp=0, key="output", trys=0, ask_for_help=1):
    try:
        output = json.loads(string)
        if "output" in output: 
            return output["output"]
    except Exception as e:
        prompt = f"I tried to parse some json and got this error: {e}. This was the would-be json.\n\n{string}\n\nReformat it to fix the error."
        if trys <= 3:
            if trys == 0:
                warm_up = 0
            else:
                warm_up = 0.25
            output = complete_text(prompt, temp=0 + warm_up, trys=trys + 1)
            print("\n" + str(output) + "\n")
        elif ask_for_help == 1:
            print(prompt + "\nReforming FAILED!!!")
            try:
                os.system("say hey! I need some help. A little help please?")
            except:
                print("'say' not supported.\n\n")
            output = input(
                "Let's see if we can avoid being derailed. Examine the above output and construct your own output text. Then enter it below. If the output needs to be something other than a string, e.g., a list or json, start it with `EVAL: `. If you're typing that, be very sure there's no malicious code in the output.\n")
            if output[:6] == "EVAL: ":
                output = eval(output[6:])
        else:
            output = "There was an error getting a response!"

    return output

# Initialize lists to store dictionaries representing rows of data
data = []

# Set the temperature for LLM model. Here use temperature as 0 to avoid GPT making things up
llm_temperature = 0
use_LLM = True

# Iterate over the files in the folder
for file_name in os.listdir(folder_path):
    if file_name.endswith('.pdf'):
        file_path = os.path.join(folder_path, file_name)
        
        # Read the text from the PDF file
        text = read_pdf(file_path)

        # Extract the case number using regular expression
        pattern = re.compile(r"CASE NUMBER:\s*([0-9]{4}-[0-9]{3}(?:AD|EXT)?[A-Za-z0-9\s]*?)(?=\s|$)|CASE NUMBER\s*([0-9]{4}-[0-9]{3})")
        case_no = pattern.search(text)
        case_no = (case_no.group(1) or case_no.group(2)).strip() if case_no else None

        if case_no:
            ad_ext = ""
            if "AD" in case_no:
                ad_ext = "AD"
            elif "EXT" in case_no:
                ad_ext = "EXT"
            case_no = case_no.replace("AD", "").replace("EXT", "")
            case_no += ad_ext
            # Extract the address using regular expression
            address = extract_address_with_keywords(text)
            # Extract Tax Parcel Number
            tax_parcel_number = extract_tax_parcel_number(text)
            # Extract Date at top
            date_at_top = extract_date_at_top(text)

        # Split the text into segments to avoid token limit error
        max_segment_length = 1000  # You can adjust this value based on your model's maximum token limit
        text_segments = [text[i:i+max_segment_length] for i in range(0, len(text), max_segment_length)]

        # Initialize lists to store the outputs for each segment
        request_outputs = []
        facts_outputs = []
        decision_outputs = []

        # Iterate over each text segment and process them separately
        for segment in text_segments:
            # Description of Variance Requested
            prompt_text = """Below you will be provided with the text of an order from a local zoning board of appeals responding to a variance request. You're looking to find the _description of variance requested_. That is, what the petitioner was asking for.        

Here's the text of the order. 

{}

---

Return a json object, including the outermost curly braces, where the key is "output" and the value is the _description of variance requested_. If you can't find a _description of variance requested_ in the text above, answer "none found". Be sure to use valid json, encasing keys and values in double quotes, and escaping internal quotes and special characters as needed.""".format(segment)
            
            request_output = clean_pseudo_json(complete_text(prompt_text, temp=llm_temperature))
            request_outputs.append(request_output)

            # Relevant Facts
            prompt_text = """Below you will be provided with the text of an order from a local zoning board of appeals responding to a variance request. You're looking to find the _relevant facts_. That is, what facts did the board need to know to rule on the petitioner's request.

Here's the text of the order. 

{}

---

Return a json object, including the outermost curly braces, where the key is "output" and the value is a short summary of the _relevant facts_. If you can't find _relevant facts_ in the text above, answer "none found". Be sure to use valid json, encasing keys and values in double quotes, and escaping internal quotes and special characters as needed.""".format(segment)

            facts_output = clean_pseudo_json(complete_text(prompt_text, temp=llm_temperature))
            facts_outputs.append(facts_output)

            # Reasoning & Decision
            prompt_text = """Below you will be provided with the text of an order from a local zoning board of appeals responding to a variance request. You're looking to find the board's _decision_ and _reasoning_. That is, how the board ruled on the petitioner's request and how it came to that decision.        

Here's the text of the order. 

{}

---

Return a json object, including the outermost curly braces, where the key is "output" and the value is a json object with two key-value pairs: (1) the first item has the key "reasoning" and the value is a summary of the board's _reasoning_ as stated above; and (2) the second item has the key "decision" with a value that is a one or two-word restatement of the _decision_ found above (e.g., "granted," "not granted," or "granted in part"). If you can't find the _decision_ or _reasoning_ in the text above, both values should read "none found". Be sure to use valid json, encasing keys and values in double quotes, and escaping internal quotes and special characters as needed.""".format(segment)

            decision_output = clean_pseudo_json(complete_text(prompt_text, temp=llm_temperature))
            decision_outputs.append(decision_output)

        for i, segment in enumerate(text_segments):
            request_output = None
            facts_output = None
            decision_output = None

            if i < len(request_outputs):
                try:
                    request_output = json.loads(request_outputs[i])
                except json.JSONDecodeError as e:
                    print(f"JSON decoding error for request_output {i}: {e}")
                    print("Problematic response:", request_outputs[i])
                    request_output = {"output": request_outputs[i]}

            if i < len(facts_outputs):
                try:
                    facts_output = json.loads(facts_outputs[i])
                except json.JSONDecodeError as e:
                    print(f"JSON decoding error for facts_output {i}: {e}")
                    print("Problematic response:", facts_outputs[i])
                    facts_output = {"output": facts_outputs[i]}

            if i < len(decision_outputs):
                if not isinstance(decision_outputs[i], dict):
                    try:
                        decision_output = json.loads(decision_outputs[i])
                    except json.JSONDecodeError as e:
                        print(f"JSON decoding error for decision_output {i}: {e}")
                        print("Problematic response:", decision_outputs[i])
                        decision_output = {"output": decision_outputs[i]}
                else:
                    decision_output = decision_outputs[i]

            reasoning = decision_output.get("output", {}).get("reasoning") if isinstance(decision_output, dict) else None
            decision = decision_output.get("output", {}).get("decision") if isinstance(decision_output, dict) else None

            row = {
                'FILE': file_name,
                'CASE_NO': case_no,
                'Tax Parcel Number': tax_parcel_number,
                'Date at top': date_at_top,
                'ADDRESS': address,
                'REQUEST': request_output.get("output") if isinstance(request_output, dict) else None,
                'FACTS': facts_output.get("output") if isinstance(facts_output, dict) else None,
                'REASONING': reasoning,
                'DECISION': decision
            }
            data.append(row)

# Create a DataFrame from the extracted data
df = pd.DataFrame(data)

# If you are happy with the result, extract to an Excel file
# Define the Excel file name and path
excel_file_path = r"C:\Users\nghie\Downloads\Charlotte NC 2019 Decision sample result.xlsx"

# Save DataFrame to Excel file
df.to_excel(excel_file_path, index=False)
